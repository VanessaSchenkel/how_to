{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8mW3Rd37TWLejV3QNp2wO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59e36b18af8447b197353738c0913dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4ef34a2f9b445cb950482b2132919d2",
              "IPY_MODEL_95a0ba490aeb48a7b48cbfffc0281c25",
              "IPY_MODEL_8624c3b21c7c45eea408567691d8f150"
            ],
            "layout": "IPY_MODEL_08a7fefb2c0842c1b7626c4474e7f675"
          }
        },
        "b4ef34a2f9b445cb950482b2132919d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a1ac0dff5ac4c1caac58de094784368",
            "placeholder": "​",
            "style": "IPY_MODEL_3b570a32bee2464da58d245f013c2dbf",
            "value": "Downloading: 100%"
          }
        },
        "95a0ba490aeb48a7b48cbfffc0281c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b0dee39ea6d40f5b715db3e328313cf",
            "max": 1199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4106986c6794497b8f16b02fe268514",
            "value": 1199
          }
        },
        "8624c3b21c7c45eea408567691d8f150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d1c2a2a820454a97b3edadfa877d6b",
            "placeholder": "​",
            "style": "IPY_MODEL_729d0bcee2d4476ba1d4d4bd9936f40d",
            "value": " 1.20k/1.20k [00:00&lt;00:00, 9.32kB/s]"
          }
        },
        "08a7fefb2c0842c1b7626c4474e7f675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a1ac0dff5ac4c1caac58de094784368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b570a32bee2464da58d245f013c2dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b0dee39ea6d40f5b715db3e328313cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4106986c6794497b8f16b02fe268514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36d1c2a2a820454a97b3edadfa877d6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "729d0bcee2d4476ba1d4d4bd9936f40d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7cb604e5a0c492d8b22cc93a80b0a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c11067abc00473f976970b913bb51bc",
              "IPY_MODEL_cf035304e3be4fbcac9358d0f25e7af1",
              "IPY_MODEL_7eb0a60fee0348ad9e1112b986122547"
            ],
            "layout": "IPY_MODEL_bd40c58b99f343d5ab6035496e238d54"
          }
        },
        "6c11067abc00473f976970b913bb51bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce661b7288af4302b462f1b2f73f67ff",
            "placeholder": "​",
            "style": "IPY_MODEL_7e760b697e75440898dc574cee6e1c6c",
            "value": "Downloading: 100%"
          }
        },
        "cf035304e3be4fbcac9358d0f25e7af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c0135c334974e54ade8cd66f460caf5",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3abf95661955447483225a5bc917f8a7",
            "value": 791656
          }
        },
        "7eb0a60fee0348ad9e1112b986122547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e906be790f534f21a95d963414429c98",
            "placeholder": "​",
            "style": "IPY_MODEL_1b4509b7e0134148820ad43b966d53a8",
            "value": " 792k/792k [00:00&lt;00:00, 1.20MB/s]"
          }
        },
        "bd40c58b99f343d5ab6035496e238d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce661b7288af4302b462f1b2f73f67ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e760b697e75440898dc574cee6e1c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c0135c334974e54ade8cd66f460caf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3abf95661955447483225a5bc917f8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e906be790f534f21a95d963414429c98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b4509b7e0134148820ad43b966d53a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cb6491b71ca4f06b57d213afd20451f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_163769b9eaca426dafb606b2576e5b50",
              "IPY_MODEL_3c3f6ed5218e4d42bc19eb13f91dbd5e",
              "IPY_MODEL_3ff2599e71784753beb4cb67ca77f4f2"
            ],
            "layout": "IPY_MODEL_7a99b030f2dd4467adb128f387606a05"
          }
        },
        "163769b9eaca426dafb606b2576e5b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1710c59e0c047a78eddd727b096c09b",
            "placeholder": "​",
            "style": "IPY_MODEL_fc57bcf9bbd5432d98d310397c4c9be6",
            "value": "Downloading: 100%"
          }
        },
        "3c3f6ed5218e4d42bc19eb13f91dbd5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_280cccd483204d2eac4b1e75e5fc1d69",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afca318c8f754a26b364a4ebf25febee",
            "value": 1389353
          }
        },
        "3ff2599e71784753beb4cb67ca77f4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8c5199dbf6e4a7bb5571d492aa90760",
            "placeholder": "​",
            "style": "IPY_MODEL_d52642ed1c19481781145a5615c2deb3",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 971kB/s]"
          }
        },
        "7a99b030f2dd4467adb128f387606a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1710c59e0c047a78eddd727b096c09b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc57bcf9bbd5432d98d310397c4c9be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "280cccd483204d2eac4b1e75e5fc1d69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afca318c8f754a26b364a4ebf25febee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8c5199dbf6e4a7bb5571d492aa90760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52642ed1c19481781145a5615c2deb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d93a866aea694323848f9cd66c719f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e74bf51769e48d899d5c70501ee1cba",
              "IPY_MODEL_c59d78c63c874ad29f3dab91aedb1f67",
              "IPY_MODEL_2efe2e8ec9a74078a4f50e3cd6a2e55e"
            ],
            "layout": "IPY_MODEL_909a9da9d6d8481eb24ee893ece68f04"
          }
        },
        "6e74bf51769e48d899d5c70501ee1cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bff9e76b8eca45659780f24609b40774",
            "placeholder": "​",
            "style": "IPY_MODEL_e4629e9be58f453c8f19069cbc9607cb",
            "value": "Downloading: 100%"
          }
        },
        "c59d78c63c874ad29f3dab91aedb1f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4647e715d1e243ab86d9f8a5a353566d",
            "max": 891691430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21001b43609c4d418cb2189322202ab9",
            "value": 891691430
          }
        },
        "2efe2e8ec9a74078a4f50e3cd6a2e55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29bdcc5024994d8aa55a0c4e0f638356",
            "placeholder": "​",
            "style": "IPY_MODEL_36860de8b2d649c786438c961aa2e5de",
            "value": " 892M/892M [00:21&lt;00:00, 52.3MB/s]"
          }
        },
        "909a9da9d6d8481eb24ee893ece68f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bff9e76b8eca45659780f24609b40774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4629e9be58f453c8f19069cbc9607cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4647e715d1e243ab86d9f8a5a353566d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21001b43609c4d418cb2189322202ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29bdcc5024994d8aa55a0c4e0f638356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36860de8b2d649c786438c961aa2e5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanessaSchenkel/spacy/blob/main/how_to_use_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "OT1Ukn0vocE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate\n",
        "The class exposes generate(), which can be used for:\n",
        "\n",
        "* greedy decoding by calling greedy_search() if num_beams=1 and do_sample=False.\n",
        "multinomial sampling by calling sample() if num_beams=1 and do_sample=True.\n",
        "* beam-search decoding by calling beam_search() if num_beams>1 and do_sample=False.\n",
        "* beam-search multinomial sampling by calling beam_sample() if num_beams>1 and do_sample=True.\n",
        "* diverse beam-search decoding by calling group_beam_search(), if num_beams>1 and num_beam_groups>1.\n",
        "* constrained beam-search decoding by calling constrained_beam_search(), if constraints!=None or force_words_ids!=None.\n"
      ],
      "metadata": {
        "id": "esCDvuJWs_X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = 'VanessaSchenkel/pt-unicamp-news-t5'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "sentence = \"The doctor was tired, she had been busy all morning.\"\n",
        "input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids, num_beams=5)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PE5JRD4vFBb",
        "outputId": "27632893-76ef-47c7-cbee-eb6f0b82f3a8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava ocupada de manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktqnVk2C1EJp",
        "outputId": "c83c28bd-b706-46d6-9dc5-e16a29c88a6f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad> O médico estava cansado, estava ocupado de manhã.</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max_length** (int, optional, defaults to model.config.max_length) — The maximum length the generated tokens can have. Corresponds to the length of the input prompt + max_new_tokens. In general, prefer the use of max_new_tokens, which ignores the number of tokens in the prompt."
      ],
      "metadata": {
        "id": "YZBxmF_W-imp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=5, max_length=5)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d7mP50Zwp8S",
        "outputId": "252dbb7e-06d5-4cad-95bc-71f3cad5caf1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O médico estava cansa']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max_new_tokens** (int, optional) — The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt."
      ],
      "metadata": {
        "id": "k1yvKLCU-u3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=5, max_new_tokens=3)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-jUODbdxM1Z",
        "outputId": "388339c0-5944-4283-ae5f-303ed17c6ceb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O médico estava']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_beams** is the number of different possible sequences considered at each generation step (see beam search for more details). This increases computation time but also increases the quality of the generated text."
      ],
      "metadata": {
        "id": "sLFrrwhH6ji0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=50)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKemds_YxaJB",
        "outputId": "344305bd-e62e-47ed-cd61-587599294b4f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava ocupada de manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=2)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLopMQhLxuJc",
        "outputId": "88aa36c0-a93a-447f-db9b-3edaef3799a4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava ocupada de manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=100, num_return_sequences=3)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTw-7tLQ9V8E",
        "outputId": "908166d9-6e81-43dd-aab9-7f2ba862b0b1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=3, num_return_sequences=3)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3nGWXgq9zVm",
        "outputId": "df1acec9-bc99-4a6c-b215-44958223910c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**min_length** is the minimum number of tokens that an output text can have. Punctuation counts as a token, and some words may be made up of more than one token, so this should be slightly more than the number of words you want\n"
      ],
      "metadata": {
        "id": "Iu5F71vL6aLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=5, min_length=20)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjGsvnUrx2U4",
        "outputId": "81468fcd-6910-4c83-c7dc-990645ee830c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, ela estava ocupada de manhã em todas as manhãs do dia.']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In its most basic form, sampling means randomly picking the next word according to its conditional probability distribution.\n",
        "In **Top-p** sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution.\n",
        "\n",
        "**top_k** is only the most probable top_k words are considered for each generation step. This avoids having very improbable words pop up during text generation. How many potential answers are considered when performing sampling.\n",
        "\n",
        "**do_sample** when is True, picks words based on their conditional probability"
      ],
      "metadata": {
        "id": "EWSw-qPczEgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "outputs = model.generate(input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_p=0.92, \n",
        "    top_k=0)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5BkBuTtyM2F",
        "outputId": "10933658-c5c0-4ebf-adb5-847fe4c67aa5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava coberta toda a manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(output):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASK3zTzC0UTG",
        "outputId": "c337108a-0781-4a1c-f419-a9404f78d1ce"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupada por toda a manhã.\n",
            "1: O médico estava cansado, estava preocupada toda manhã.\n",
            "2: A médica estava cansada, ela tinha ficado ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=5, early_stopping=True)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWR2wNOX05LO",
        "outputId": "69e5cf44-58ff-472a-d7b6-af819b60a448"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava ocupada de manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_beams** returns the n most probable next words. Number of beams for beam search. 1 means no beam search.\n"
      ],
      "metadata": {
        "id": "gA7Wb-Jr1o0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=3, num_return_sequences=3)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RptXQX3d1yNY",
        "outputId": "dde044e8-7ae0-429a-8da3-21037a38fb33"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=100, num_return_sequences=3)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM_Q1rqu12Fb",
        "outputId": "f51845f2-ef61-4413-9f55-0b8e81753535"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Temperature** is a hyper-parameter used to control the randomness of predictions by scaling the logits before applying softmax."
      ],
      "metadata": {
        "id": "Om3Bpq5V3Olt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.1,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR2hwdFm_9Mk",
        "outputId": "eddd076c-1a02-49f8-9271-5ceacd16744b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupado de manhã.\n",
            "1: O médico estava cansado, estava ocupado de manhã.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.7,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRjIMEsa3Uhu",
        "outputId": "d5b3e85b-fd7e-46b3-a1ba-f532c4b2d7dd"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, está todas as manhãs ocupadas.\n",
            "1: A médica estava cansada, estava ocupada de manhã.\n",
            "2: O médico estava cansado, ela esteve ocupada durante toda a manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.9,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt5YRYDH_3gX",
        "outputId": "789e252d-758d-4f65-f7f4-0b8af4d1ad24"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, inteirava-se de manhã.\n",
            "1: O médico estava cansado, estava habituado pela manhã.\n",
            "2: A médica estava cansada, estava atendia de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **repetition_penalty** is meant to avoid sentences that repeat, can be used to penalize words that were already generated or belong to the context."
      ],
      "metadata": {
        "id": "wu3WI_o236WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    repetition_penalty=0.1,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ_Mb6Wq35nE",
        "outputId": "262fe277-e5ca-49b9-96a3-79b664e9e970"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    repetition_penalty=0.5,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPDlh4pgBzJ8",
        "outputId": "d6a531de-b2d6-4b5f-e65d-85f974ef464f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupado, de manhã, de manhã, de manhã, de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    repetition_penalty=5.0,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsl8yzKzBuIQ",
        "outputId": "3760b5a0-76ce-43b7-9b45-0586fae3a045"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, tinha sido ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    repetition_penalty=10.0,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iSzY_cg4dKD",
        "outputId": "eb8af4a1-f90d-4980-ad76-c3416c3e72c6"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, tinha sido ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**length_penalty** (float, optional, defaults to model.config.length_penalty or 1.0 if the config does not set any value) — Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length. 0.0 means no penalty. Set to values < 0.0 in order to encourage the model to generate longer sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences."
      ],
      "metadata": {
        "id": "hJWgX8Y14wuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    length_penalty=-10.0,\n",
        "    max_length=2\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qsJDduW4ucK",
        "outputId": "9bfb4adb-7ed1-4c9b-c02d-b5ab4b6e69e6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    length_penalty=10.0,\n",
        "    max_length=5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGbLAtob5FPS",
        "outputId": "b843690f-da7c-4f15-f090-f22730382412"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    length_penalty=10.0,\n",
        "    max_length=2\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30PGy-sr5bA_",
        "outputId": "15d567de-5b44-4b2d-f589-7bf301f432b6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**no_repeat_ngram_size** avoids repetition of n_grams (sequence of n consecutive words). This is useful when producing longer texts, as models sometimes repeat themselves : in this case I suggest using a value of 3 or 4 to ensure diversity without hurting performance."
      ],
      "metadata": {
        "id": "65pT9JK165PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    no_repeat_ngram_size=1,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrPQHmsw7CS1",
        "outputId": "1d08ac26-85ca-4a83-db9e-1db1209ce36a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, tinha sido ocupada de manhã.\n",
            "1: O médico estava cansado, tinha sido ocupado de manhã.\n",
            "2: A médica estava cansada, ela tinha sido ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    no_repeat_ngram_size=4,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzQxYvvWDA8k",
        "outputId": "14cd8b06-b677-46f8-8304-ca258004f799"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    no_repeat_ngram_size=100,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0_lAcBw7Lh9",
        "outputId": "2943cdff-c5a2-48da-c45f-2f8ea985c58e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    no_repeat_ngram_size=5,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYstiiyHDLF9",
        "outputId": "9f58c03c-0a00-4e48-9e9a-8c443ea38ecf"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: O médico estava cansado, estava ocupado de manhã.\n",
            "2: A médica estava cansada, estava ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**encoder_no_repeat_ngram_size** if set to int > 0, all ngrams of that size that occur in the encoder_input_ids cannot occur in the decoder_input_ids."
      ],
      "metadata": {
        "id": "cFXssMF3Hlr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    encoder_no_repeat_ngram_size=1\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CqOkM69HF4p",
        "outputId": "e28bca29-ec47-4978-8ad6-e59ba903d7ed"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada e ela estava ocupada de manhã em todas as suas atividades de dia para\n",
            "1: O médico estava cansado e ela estava ocupada de manhã em todas as suas atividades de dia para\n",
            "2: A médica estava cansada e ela estava ocupada de manhã em todas as suas atividades de dia-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    encoder_no_repeat_ngram_size=10\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7JH_W-2HSp5",
        "outputId": "3dfa6f1c-1ece-4701-c1d7-7d21ff56f2b5"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=100,\n",
        "    num_return_sequences=3,\n",
        "    encoder_no_repeat_ngram_size=100\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4rVC9PEHXha",
        "outputId": "3d9620d0-4a6b-40bd-afac-ab38dca0e930"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bad_words_ids** is a list of token ids that are not allowed to be generated. In order to get the token ids of the words that should not appear in the generated text, use tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids."
      ],
      "metadata": {
        "id": "-SGNJ1o9Hxy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = [\"cansado\"]\n",
        "\n",
        "bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    bad_words_ids=bad_words_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDM628JHIH8t",
        "outputId": "7d6d5995-b296-4eee-a41a-b3e0bcaa2419"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: A médica estava cansada, estava ocupada toda a manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = [\"o\", \"médico\"]\n",
        "\n",
        "bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    bad_words_ids=bad_words_ids,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmKmPeSFK_qA",
        "outputId": "c39b6476-d482-4e4f-b253-43f869ef6794"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O doutor estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**force_words_ids** is a list of token ids that must be generated. If given a List[List[int]], this is treated as a simple list of words that must be included, the opposite to bad_words_ids. If given List[List[List[int]]], this triggers a disjunctive constraint, where one can allow different forms of each word."
      ],
      "metadata": {
        "id": "0LR6iL9uL8oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "force_words = [\"a\", \"médica\"]\n",
        "\n",
        "force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    force_words_ids=force_words_ids,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZQDPs7qMJTh",
        "outputId": "84c169c9-434c-4374-b6b3-0f87beea5cf2"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava toda a manhã ocupada.\n",
            "1: A médica estava cansada, estava ocupada a toda manhã.\n",
            "2: A médica estava cansada, ela estava ocupada de manhã em toda a manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max_time** is the maximum amount of time you allow the computation to run for in seconds. generation will still finish the current pass after allocated time has been passed.\n"
      ],
      "metadata": {
        "id": "_kuiMGaLNZbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "force_words = [\"a\", \"médica\"]\n",
        "\n",
        "force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    force_words_ids=force_words_ids,\n",
        "    max_time=1.0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erpQtGX_NYtS",
        "outputId": "1d18501c-a0ed-4866-d8c5-aa51b4700cfc"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Tinha a médica\n",
            "1: A médica a\n",
            "2: O médica a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_beam_groups** is the number of groups to divide num_beams into in order to ensure diversity among different groups of beams."
      ],
      "metadata": {
        "id": "q7NXfoZSOcR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=2\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ix1qp7NOamu",
        "outputId": "f1af405e-9862-47a7-8971-d61a1b52ec96"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava ocupada de manhã.\n",
            "2: A médica estava cansada, estava toda manhã ocupada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJgArWsfO21b",
        "outputId": "7698fbf2-313c-4b75-a3f7-5136db5124d5"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava ocupada de manhã.\n",
            "2: A médica estava cansada, estava ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=10\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWHVdNoWOwcr",
        "outputId": "4b3605d4-8810-4ada-fb19-6ba59524d70f"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupado de manhã.\n",
            "1: O médico estava cansado, estava ocupado de manhã.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**diversity_penalty** is a value is subtracted from a beam’s score if it generates a token same as any beam from other group at a particular time. Note that diversity_penalty is only effective if group beam search is enabled."
      ],
      "metadata": {
        "id": "fudBtMPyPE6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=2,\n",
        "    diversity_penalty=1.5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-kHEPRoPMD-",
        "outputId": "913cb388-dc08-4767-d4cc-b02aaf5502ae"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=2,\n",
        "    diversity_penalty=50.0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGZeGL3_PoXC",
        "outputId": "ded8f67a-218f-4e4e-c0ea-f7f90a9ba4c0"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=5,\n",
        "    num_beam_groups = 5,\n",
        "    num_return_sequences=5,\n",
        "    diversity_penalty = 0.70\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxCwuIoCPySU",
        "outputId": "fa7bd445-a365-41ea-9d99-78039683f2d8"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupado de manhã.\n",
            "1: A médica estava cansada, estava ocupada de manhã.\n",
            "2: O médico estava cansado, tinha sido ocupado de manhã.\n",
            "3: O médico estava cansado, estava ocupado de manhã...\n",
            "4: A médica estava cansada, ela estava ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prefix_allowed_tokens_fn** if provided, this function constraints the beam search to allowed tokens only at each step. If not provided no constraint is applied. This function takes 2 arguments: the batch ID batch_id and input_ids. It has to return a list with the allowed tokens for the next generation step conditioned on the batch ID batch_id and the previously generated tokens inputs_ids. This argument is useful for constrained generation conditioned on the prefix, as described in Autoregressive Entity Retrieval."
      ],
      "metadata": {
        "id": "pBQ4RseeQ7nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**logits_processor** (LogitsProcessorList, optional) — Custom logits processors that complement the default logits processors built from arguments and a model’s config. If a logit processor is passed that is already created with the arguments or a model’s config an error is thrown. This feature is intended for advanced users. renormalize_logits — (bool, optional, defaults to False): Whether to renormalize the logits after applying all the logits processors or warpers (including the custom ones). It’s highly recommended to set this flag to True as the search algorithms suppose the score logits are normalized but some logit processors or warpers break the normalization."
      ],
      "metadata": {
        "id": "VEzXSTDxVnUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    renormalize_logits=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghuI0LDtVmHs",
        "outputId": "7e92d79c-3f89-42d7-e79d-dba0ea5050a1"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**constraints** is a custom constraints that can be added to the generation to ensure that the output will contain the use of certain tokens as defined by Constraint objects, in the most sensible way possible."
      ],
      "metadata": {
        "id": "4br_et8sWJmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PhrasalConstraint\n",
        "\n",
        "constraints = [\n",
        "    PhrasalConstraint(\n",
        "        tokenizer(\"a médica\", add_special_tokens=False).input_ids\n",
        "    )\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    constraints=constraints,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpTv0ZNQXHFO",
        "outputId": "b1bde75a-cadc-40bb-e549-9f0074f7a12c"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã a toda a manhã, e a médica estava\n",
            "1: A médica estava cansada, estava ocupada de manhã a toda a manhã... a médica\n",
            "2: A médica estava cansada, estava ocupada de manhã a toda a manhã. A médica a médica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PhrasalConstraint\n",
        "\n",
        "constraints = [\n",
        "    PhrasalConstraint(\n",
        "        tokenizer(\"médica\", add_special_tokens=False).input_ids\n",
        "    )\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    constraints=constraints,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LveU-ZPKXy4A",
        "outputId": "a5bf278b-4292-4a4e-de62-5b8f1d74d73e"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: A médica estava cansada, estava ocupada toda a manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DisjunctiveConstraint\n",
        "\n",
        "flexible_phrases = tokenizer(\n",
        "            [\"médica\", \"médico\"], add_special_tokens=False\n",
        "        ).input_ids\n",
        "\n",
        "constraints = [DisjunctiveConstraint(flexible_phrases)]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    constraints=constraints,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7lL00eFYUR2",
        "outputId": "0c20b6a9-20ed-49e9-81db-750e8fa32b81"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contrained beam search\n",
        "\n",
        "Generates sequences of token ids for models with a language modeling head using constrained beam search decoding and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
        "\n",
        "Params:\n",
        "* input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — The sequence used as a prompt for the generation.\n",
        "* constrained_beam_scorer (ConstrainedBeamSearchScorer) — A derived instance of BeamScorer that defines how beam hypotheses are constructed, stored and sorted during generation, while satisfying a list of positive constraints. For more information, the documentation of ConstrainedBeamSearchScorer should be read.\n",
        "* logits_processor (LogitsProcessorList, optional) — An instance of LogitsProcessorList. List of instances of class derived from LogitsProcessor used to modify the prediction scores of the language modeling head applied at each generation step.\n",
        "* stopping_criteria (StoppingCriteriaList, optional) — An instance of StoppingCriteriaList. List of instances of class derived from StoppingCriteria used to tell if the generation loop should stop.\n",
        "* logits_warper (LogitsProcessorList, optional) — An instance of LogitsProcessorList. List of instances of class derived from LogitsWarper used to warp the prediction score distribution of the language modeling head applied before multinomial sampling at each generation step.\n",
        "* max_length (int, optional, defaults to 20) — DEPRECATED. Use logits_processor or stopping_criteria directly to cap the number of generated tokens. The maximum length of the sequence to be generated.\n",
        "* pad_token_id (int, optional) — The id of the padding token.\n",
        "* eos_token_id (int, optional) — The id of the end-of-sequence token.\n",
        "* output_attentions (bool, optional, defaults to False) — Whether or not to return the attentions tensors of all attention layers. See attentions under returned tensors for more details.\n",
        "* output_hidden_states (bool, optional, defaults to False) — Whether or not to return the hidden states of all layers. See hidden_states under returned tensors for more details.\n",
        "* output_scores (bool, optional, defaults to False) — Whether or not to return the prediction scores. See scores under returned tensors for more details.\n",
        "* return_dict_in_generate (bool, optional, defaults to False) — Whether or not to return a ModelOutput instead of a plain tuple.\n",
        "* synced_gpus (bool, optional, defaults to False) — Whether to continue running the while loop until max_length (needed for ZeRO stage 3) * model_kwargs — Additional model specific kwargs will be forwarded to the forward function of the model. If model is an encoder-decoder model the kwargs should include encoder_outputs.\n"
      ],
      "metadata": {
        "id": "eUNdw8SAnjv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    LogitsProcessorList,\n",
        "    MinLengthLogitsProcessor,\n",
        "    ConstrainedBeamSearchScorer,\n",
        "    PhrasalConstraint,\n",
        ")\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "\n",
        "encoder_input_str = \"translate English to German: How old are you?\"\n",
        "encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "# lets run beam search using 3 beams\n",
        "num_beams = 3\n",
        "# define decoder start token ids\n",
        "input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
        "input_ids = input_ids * model.config.decoder_start_token_id\n",
        "\n",
        "# add encoder_outputs to model keyword arguments\n",
        "model_kwargs = {\n",
        "    \"encoder_outputs\": model.get_encoder()(\n",
        "        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
        "    )\n",
        "}\n",
        "\n",
        "constraint_str = \"Sie\"\n",
        "constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token\n",
        "constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]\n",
        "\n",
        "\n",
        "# instantiate beam scorer\n",
        "beam_scorer = ConstrainedBeamSearchScorer(\n",
        "    batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints\n",
        ")\n",
        "\n",
        "# instantiate logits processors\n",
        "logits_processor = LogitsProcessorList(\n",
        "    [\n",
        "        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
        "    ]\n",
        ")\n",
        "\n",
        "outputs = model.constrained_beam_search(\n",
        "    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs\n",
        ")\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325,
          "referenced_widgets": [
            "59e36b18af8447b197353738c0913dd2",
            "b4ef34a2f9b445cb950482b2132919d2",
            "95a0ba490aeb48a7b48cbfffc0281c25",
            "8624c3b21c7c45eea408567691d8f150",
            "08a7fefb2c0842c1b7626c4474e7f675",
            "3a1ac0dff5ac4c1caac58de094784368",
            "3b570a32bee2464da58d245f013c2dbf",
            "9b0dee39ea6d40f5b715db3e328313cf",
            "b4106986c6794497b8f16b02fe268514",
            "36d1c2a2a820454a97b3edadfa877d6b",
            "729d0bcee2d4476ba1d4d4bd9936f40d",
            "d7cb604e5a0c492d8b22cc93a80b0a18",
            "6c11067abc00473f976970b913bb51bc",
            "cf035304e3be4fbcac9358d0f25e7af1",
            "7eb0a60fee0348ad9e1112b986122547",
            "bd40c58b99f343d5ab6035496e238d54",
            "ce661b7288af4302b462f1b2f73f67ff",
            "7e760b697e75440898dc574cee6e1c6c",
            "3c0135c334974e54ade8cd66f460caf5",
            "3abf95661955447483225a5bc917f8a7",
            "e906be790f534f21a95d963414429c98",
            "1b4509b7e0134148820ad43b966d53a8",
            "8cb6491b71ca4f06b57d213afd20451f",
            "163769b9eaca426dafb606b2576e5b50",
            "3c3f6ed5218e4d42bc19eb13f91dbd5e",
            "3ff2599e71784753beb4cb67ca77f4f2",
            "7a99b030f2dd4467adb128f387606a05",
            "d1710c59e0c047a78eddd727b096c09b",
            "fc57bcf9bbd5432d98d310397c4c9be6",
            "280cccd483204d2eac4b1e75e5fc1d69",
            "afca318c8f754a26b364a4ebf25febee",
            "c8c5199dbf6e4a7bb5571d492aa90760",
            "d52642ed1c19481781145a5615c2deb3",
            "d93a866aea694323848f9cd66c719f74",
            "6e74bf51769e48d899d5c70501ee1cba",
            "c59d78c63c874ad29f3dab91aedb1f67",
            "2efe2e8ec9a74078a4f50e3cd6a2e55e",
            "909a9da9d6d8481eb24ee893ece68f04",
            "bff9e76b8eca45659780f24609b40774",
            "e4629e9be58f453c8f19069cbc9607cb",
            "4647e715d1e243ab86d9f8a5a353566d",
            "21001b43609c4d418cb2189322202ab9",
            "29bdcc5024994d8aa55a0c4e0f638356",
            "36860de8b2d649c786438c961aa2e5de"
          ]
        },
        "id": "LirdmO1gnpJR",
        "outputId": "935ae6d4-e31d-40a8-e701-8be894914f39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59e36b18af8447b197353738c0913dd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7cb604e5a0c492d8b22cc93a80b0a18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cb6491b71ca4f06b57d213afd20451f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d93a866aea694323848f9cd66c719f74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:3174: UserWarning: You don't have defined any stopping_criteria, this will likely loop forever\n",
            "  warnings.warn(\"You don't have defined any stopping_criteria, this will likely loop forever\", UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wie alt sind Sie?']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import numpy as np\n",
        "import math\n",
        "import heapq\n",
        " \n",
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "    \"\"\"\n",
        "    data: (n, m) where n is number of words in sequence.\n",
        "        and m is number of classes (words in target vocab).\n",
        "    k: beam search parameter\n",
        "    \"\"\"\n",
        "    sequences = [[[], 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data: # ----> n\n",
        "        all_candidates = []\n",
        "        # find the indexes of k largest probabilities in the row\n",
        "        k_largest = heapq.nlargest(k, range(len(row)), row.take) # -----> m\n",
        "        # expand each current candidate\n",
        "        for seq, score in sequences: # ----> k\n",
        "            for j in k_largest: # -----> k\n",
        "                s = score - math.log(row[j])\n",
        "                candidate = [seq + [j], s]\n",
        "                all_candidates.append(candidate)\n",
        "        # sort all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1]) # -----> k log k\n",
        "        # select best k\n",
        "        sequences = ordered[:k]\n",
        "    return sequences\n",
        " \n",
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1]]\n",
        "data = array(data)\n",
        "# decode sequence\n",
        "result = beam_search_decoder(data, 3)\n",
        "# print result\n",
        "for seq in result:\n",
        "\tprint(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3_iemY4sFUF",
        "outputId": "c384f1b9-4632-4d35-f06f-cae3bffb8b69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]\n"
          ]
        }
      ]
    }
  ]
}