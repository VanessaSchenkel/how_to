{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNoH6sg5enFFysAopK19+Ai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanessaSchenkel/how_to/blob/main/how_to_use_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "OT1Ukn0vocE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate\n",
        "The class exposes generate(), which can be used for:\n",
        "\n",
        "* greedy decoding by calling greedy_search() if num_beams=1 and do_sample=False.\n",
        "multinomial sampling by calling sample() if num_beams=1 and do_sample=True.\n",
        "* beam-search decoding by calling beam_search() if num_beams>1 and do_sample=False.\n",
        "* beam-search multinomial sampling by calling beam_sample() if num_beams>1 and do_sample=True.\n",
        "* diverse beam-search decoding by calling group_beam_search(), if num_beams>1 and num_beam_groups>1.\n",
        "* constrained beam-search decoding by calling constrained_beam_search(), if constraints!=None or force_words_ids!=None.\n"
      ],
      "metadata": {
        "id": "esCDvuJWs_X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = 'VanessaSchenkel/pt-unicamp-news-t5'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "sentence = \"The doctor was tired, she had been busy all morning.\"\n",
        "input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids, num_beams=5)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PE5JRD4vFBb",
        "outputId": "27632893-76ef-47c7-cbee-eb6f0b82f3a8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava ocupada de manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktqnVk2C1EJp",
        "outputId": "c83c28bd-b706-46d6-9dc5-e16a29c88a6f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad> O médico estava cansado, estava ocupado de manhã.</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max_length** (int, optional, defaults to model.config.max_length) — The maximum length the generated tokens can have. Corresponds to the length of the input prompt + max_new_tokens. In general, prefer the use of max_new_tokens, which ignores the number of tokens in the prompt."
      ],
      "metadata": {
        "id": "YZBxmF_W-imp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=5, max_length=5)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d7mP50Zwp8S",
        "outputId": "252dbb7e-06d5-4cad-95bc-71f3cad5caf1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O médico estava cansa']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max_new_tokens** (int, optional) — The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt."
      ],
      "metadata": {
        "id": "k1yvKLCU-u3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=5, max_new_tokens=3)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-jUODbdxM1Z",
        "outputId": "388339c0-5944-4283-ae5f-303ed17c6ceb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O médico estava']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_beams** is the number of different possible sequences considered at each generation step (see beam search for more details). This increases computation time but also increases the quality of the generated text."
      ],
      "metadata": {
        "id": "sLFrrwhH6ji0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=50)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKemds_YxaJB",
        "outputId": "344305bd-e62e-47ed-cd61-587599294b4f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava ocupada de manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=2)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLopMQhLxuJc",
        "outputId": "88aa36c0-a93a-447f-db9b-3edaef3799a4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava ocupada de manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=100, num_return_sequences=3)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTw-7tLQ9V8E",
        "outputId": "908166d9-6e81-43dd-aab9-7f2ba862b0b1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=3, num_return_sequences=3)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3nGWXgq9zVm",
        "outputId": "df1acec9-bc99-4a6c-b215-44958223910c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**min_length** is the minimum number of tokens that an output text can have. Punctuation counts as a token, and some words may be made up of more than one token, so this should be slightly more than the number of words you want\n"
      ],
      "metadata": {
        "id": "Iu5F71vL6aLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=5, min_length=20)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjGsvnUrx2U4",
        "outputId": "81468fcd-6910-4c83-c7dc-990645ee830c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, ela estava ocupada de manhã em todas as manhãs do dia.']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In its most basic form, sampling means randomly picking the next word according to its conditional probability distribution.\n",
        "In **Top-p** sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution.\n",
        "\n",
        "**top_k** is only the most probable top_k words are considered for each generation step. This avoids having very improbable words pop up during text generation. How many potential answers are considered when performing sampling.\n",
        "\n",
        "**do_sample** when is True, picks words based on their conditional probability"
      ],
      "metadata": {
        "id": "EWSw-qPczEgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "outputs = model.generate(input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_p=0.92, \n",
        "    top_k=0)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5BkBuTtyM2F",
        "outputId": "10933658-c5c0-4ebf-adb5-847fe4c67aa5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava coberta toda a manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(output):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASK3zTzC0UTG",
        "outputId": "c337108a-0781-4a1c-f419-a9404f78d1ce"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupada por toda a manhã.\n",
            "1: O médico estava cansado, estava preocupada toda manhã.\n",
            "2: A médica estava cansada, ela tinha ficado ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=5, early_stopping=True)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWR2wNOX05LO",
        "outputId": "69e5cf44-58ff-472a-d7b6-af819b60a448"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A médica estava cansada, estava ocupada de manhã.']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_beams** returns the n most probable next words. Number of beams for beam search. 1 means no beam search.\n"
      ],
      "metadata": {
        "id": "gA7Wb-Jr1o0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=3, num_return_sequences=3)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RptXQX3d1yNY",
        "outputId": "dde044e8-7ae0-429a-8da3-21037a38fb33"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids, num_beams=100, num_return_sequences=3)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM_Q1rqu12Fb",
        "outputId": "f51845f2-ef61-4413-9f55-0b8e81753535"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Temperature** is a hyper-parameter used to control the randomness of predictions by scaling the logits before applying softmax."
      ],
      "metadata": {
        "id": "Om3Bpq5V3Olt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.1,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR2hwdFm_9Mk",
        "outputId": "eddd076c-1a02-49f8-9271-5ceacd16744b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupado de manhã.\n",
            "1: O médico estava cansado, estava ocupado de manhã.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.7,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRjIMEsa3Uhu",
        "outputId": "d5b3e85b-fd7e-46b3-a1ba-f532c4b2d7dd"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, está todas as manhãs ocupadas.\n",
            "1: A médica estava cansada, estava ocupada de manhã.\n",
            "2: O médico estava cansado, ela esteve ocupada durante toda a manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.9,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt5YRYDH_3gX",
        "outputId": "789e252d-758d-4f65-f7f4-0b8af4d1ad24"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, inteirava-se de manhã.\n",
            "1: O médico estava cansado, estava habituado pela manhã.\n",
            "2: A médica estava cansada, estava atendia de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **repetition_penalty** is meant to avoid sentences that repeat, can be used to penalize words that were already generated or belong to the context."
      ],
      "metadata": {
        "id": "wu3WI_o236WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    repetition_penalty=0.1,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ_Mb6Wq35nE",
        "outputId": "262fe277-e5ca-49b9-96a3-79b664e9e970"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    repetition_penalty=0.5,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPDlh4pgBzJ8",
        "outputId": "d6a531de-b2d6-4b5f-e65d-85f974ef464f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupado, de manhã, de manhã, de manhã, de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    repetition_penalty=5.0,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsl8yzKzBuIQ",
        "outputId": "3760b5a0-76ce-43b7-9b45-0586fae3a045"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, tinha sido ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    repetition_penalty=10.0,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iSzY_cg4dKD",
        "outputId": "eb8af4a1-f90d-4980-ad76-c3416c3e72c6"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, tinha sido ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**length_penalty** (float, optional, defaults to model.config.length_penalty or 1.0 if the config does not set any value) — Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length. 0.0 means no penalty. Set to values < 0.0 in order to encourage the model to generate longer sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences."
      ],
      "metadata": {
        "id": "hJWgX8Y14wuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    length_penalty=-10.0,\n",
        "    max_length=2\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qsJDduW4ucK",
        "outputId": "9bfb4adb-7ed1-4c9b-c02d-b5ab4b6e69e6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    length_penalty=10.0,\n",
        "    max_length=5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGbLAtob5FPS",
        "outputId": "b843690f-da7c-4f15-f090-f22730382412"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    length_penalty=10.0,\n",
        "    max_length=2\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30PGy-sr5bA_",
        "outputId": "15d567de-5b44-4b2d-f589-7bf301f432b6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**no_repeat_ngram_size** avoids repetition of n_grams (sequence of n consecutive words). This is useful when producing longer texts, as models sometimes repeat themselves : in this case I suggest using a value of 3 or 4 to ensure diversity without hurting performance."
      ],
      "metadata": {
        "id": "65pT9JK165PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    no_repeat_ngram_size=1,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrPQHmsw7CS1",
        "outputId": "1d08ac26-85ca-4a83-db9e-1db1209ce36a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, tinha sido ocupada de manhã.\n",
            "1: O médico estava cansado, tinha sido ocupado de manhã.\n",
            "2: A médica estava cansada, ela tinha sido ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    no_repeat_ngram_size=4,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzQxYvvWDA8k",
        "outputId": "14cd8b06-b677-46f8-8304-ca258004f799"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    no_repeat_ngram_size=100,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0_lAcBw7Lh9",
        "outputId": "2943cdff-c5a2-48da-c45f-2f8ea985c58e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    no_repeat_ngram_size=5,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYstiiyHDLF9",
        "outputId": "9f58c03c-0a00-4e48-9e9a-8c443ea38ecf"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: O médico estava cansado, estava ocupado de manhã.\n",
            "2: A médica estava cansada, estava ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**encoder_no_repeat_ngram_size** if set to int > 0, all ngrams of that size that occur in the encoder_input_ids cannot occur in the decoder_input_ids."
      ],
      "metadata": {
        "id": "cFXssMF3Hlr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    encoder_no_repeat_ngram_size=1\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CqOkM69HF4p",
        "outputId": "e28bca29-ec47-4978-8ad6-e59ba903d7ed"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada e ela estava ocupada de manhã em todas as suas atividades de dia para\n",
            "1: O médico estava cansado e ela estava ocupada de manhã em todas as suas atividades de dia para\n",
            "2: A médica estava cansada e ela estava ocupada de manhã em todas as suas atividades de dia-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    encoder_no_repeat_ngram_size=10\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7JH_W-2HSp5",
        "outputId": "3dfa6f1c-1ece-4701-c1d7-7d21ff56f2b5"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=100,\n",
        "    num_return_sequences=3,\n",
        "    encoder_no_repeat_ngram_size=100\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4rVC9PEHXha",
        "outputId": "3d9620d0-4a6b-40bd-afac-ab38dca0e930"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bad_words_ids** is a list of token ids that are not allowed to be generated. In order to get the token ids of the words that should not appear in the generated text, use tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids."
      ],
      "metadata": {
        "id": "-SGNJ1o9Hxy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = [\"cansado\"]\n",
        "\n",
        "bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    bad_words_ids=bad_words_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDM628JHIH8t",
        "outputId": "7d6d5995-b296-4eee-a41a-b3e0bcaa2419"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: A médica estava cansada, estava ocupada toda a manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = [\"o\", \"médico\"]\n",
        "\n",
        "bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    bad_words_ids=bad_words_ids,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmKmPeSFK_qA",
        "outputId": "c39b6476-d482-4e4f-b253-43f869ef6794"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O doutor estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**force_words_ids** is a list of token ids that must be generated. If given a List[List[int]], this is treated as a simple list of words that must be included, the opposite to bad_words_ids. If given List[List[List[int]]], this triggers a disjunctive constraint, where one can allow different forms of each word."
      ],
      "metadata": {
        "id": "0LR6iL9uL8oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "force_words = [\"a\", \"médica\"]\n",
        "\n",
        "force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    force_words_ids=force_words_ids,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZQDPs7qMJTh",
        "outputId": "84c169c9-434c-4374-b6b3-0f87beea5cf2"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava toda a manhã ocupada.\n",
            "1: A médica estava cansada, estava ocupada a toda manhã.\n",
            "2: A médica estava cansada, ela estava ocupada de manhã em toda a manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max_time** is the maximum amount of time you allow the computation to run for in seconds. generation will still finish the current pass after allocated time has been passed.\n"
      ],
      "metadata": {
        "id": "_kuiMGaLNZbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "force_words = [\"a\", \"médica\"]\n",
        "\n",
        "force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    force_words_ids=force_words_ids,\n",
        "    max_time=1.0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erpQtGX_NYtS",
        "outputId": "1d18501c-a0ed-4866-d8c5-aa51b4700cfc"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Tinha a médica\n",
            "1: A médica a\n",
            "2: O médica a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_beam_groups** is the number of groups to divide num_beams into in order to ensure diversity among different groups of beams."
      ],
      "metadata": {
        "id": "q7NXfoZSOcR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=2\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ix1qp7NOamu",
        "outputId": "f1af405e-9862-47a7-8971-d61a1b52ec96"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava ocupada de manhã.\n",
            "2: A médica estava cansada, estava toda manhã ocupada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJgArWsfO21b",
        "outputId": "7698fbf2-313c-4b75-a3f7-5136db5124d5"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava ocupada de manhã.\n",
            "2: A médica estava cansada, estava ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=10\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWHVdNoWOwcr",
        "outputId": "4b3605d4-8810-4ada-fb19-6ba59524d70f"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupado de manhã.\n",
            "1: O médico estava cansado, estava ocupado de manhã.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**diversity_penalty** is a value is subtracted from a beam’s score if it generates a token same as any beam from other group at a particular time. Note that diversity_penalty is only effective if group beam search is enabled."
      ],
      "metadata": {
        "id": "fudBtMPyPE6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=2,\n",
        "    diversity_penalty=1.5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-kHEPRoPMD-",
        "outputId": "913cb388-dc08-4767-d4cc-b02aaf5502ae"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    num_beam_groups=2,\n",
        "    diversity_penalty=50.0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGZeGL3_PoXC",
        "outputId": "ded8f67a-218f-4e4e-c0ea-f7f90a9ba4c0"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=5,\n",
        "    num_beam_groups = 5,\n",
        "    num_return_sequences=5,\n",
        "    diversity_penalty = 0.70\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxCwuIoCPySU",
        "outputId": "fa7bd445-a365-41ea-9d99-78039683f2d8"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: O médico estava cansado, estava ocupado de manhã.\n",
            "1: A médica estava cansada, estava ocupada de manhã.\n",
            "2: O médico estava cansado, tinha sido ocupado de manhã.\n",
            "3: O médico estava cansado, estava ocupado de manhã...\n",
            "4: A médica estava cansada, ela estava ocupada de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prefix_allowed_tokens_fn** if provided, this function constraints the beam search to allowed tokens only at each step. If not provided no constraint is applied. This function takes 2 arguments: the batch ID batch_id and input_ids. It has to return a list with the allowed tokens for the next generation step conditioned on the batch ID batch_id and the previously generated tokens inputs_ids. This argument is useful for constrained generation conditioned on the prefix, as described in Autoregressive Entity Retrieval."
      ],
      "metadata": {
        "id": "pBQ4RseeQ7nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**logits_processor** (LogitsProcessorList, optional) — Custom logits processors that complement the default logits processors built from arguments and a model’s config. If a logit processor is passed that is already created with the arguments or a model’s config an error is thrown. This feature is intended for advanced users. renormalize_logits — (bool, optional, defaults to False): Whether to renormalize the logits after applying all the logits processors or warpers (including the custom ones). It’s highly recommended to set this flag to True as the search algorithms suppose the score logits are normalized but some logit processors or warpers break the normalization."
      ],
      "metadata": {
        "id": "VEzXSTDxVnUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    renormalize_logits=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghuI0LDtVmHs",
        "outputId": "7e92d79c-3f89-42d7-e79d-dba0ea5050a1"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**constraints** is a custom constraints that can be added to the generation to ensure that the output will contain the use of certain tokens as defined by Constraint objects, in the most sensible way possible."
      ],
      "metadata": {
        "id": "4br_et8sWJmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PhrasalConstraint\n",
        "\n",
        "constraints = [\n",
        "    PhrasalConstraint(\n",
        "        tokenizer(\"a médica\", add_special_tokens=False).input_ids\n",
        "    )\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    constraints=constraints,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpTv0ZNQXHFO",
        "outputId": "b1bde75a-cadc-40bb-e549-9f0074f7a12c"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã a toda a manhã, e a médica estava\n",
            "1: A médica estava cansada, estava ocupada de manhã a toda a manhã... a médica\n",
            "2: A médica estava cansada, estava ocupada de manhã a toda a manhã. A médica a médica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PhrasalConstraint\n",
        "\n",
        "constraints = [\n",
        "    PhrasalConstraint(\n",
        "        tokenizer(\"médica\", add_special_tokens=False).input_ids\n",
        "    )\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    constraints=constraints,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LveU-ZPKXy4A",
        "outputId": "a5bf278b-4292-4a4e-de62-5b8f1d74d73e"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: A médica estava cansada, estava ocupada toda a manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DisjunctiveConstraint\n",
        "\n",
        "flexible_phrases = tokenizer(\n",
        "            [\"médica\", \"médico\"], add_special_tokens=False\n",
        "        ).input_ids\n",
        "\n",
        "constraints = [DisjunctiveConstraint(flexible_phrases)]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    constraints=constraints,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7lL00eFYUR2",
        "outputId": "0c20b6a9-20ed-49e9-81db-750e8fa32b81"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DisjunctiveConstraint\n",
        "\n",
        "flexible_phrases = tokenizer(\n",
        "            [\"médica\", \"médico\", \"doutora\"], add_special_tokens=False\n",
        "        ).input_ids\n",
        "\n",
        "constraints = [DisjunctiveConstraint(flexible_phrases)]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    constraints=constraints,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjxJakpoJ5q6",
        "outputId": "cf1735f1-5f84-41fd-f8b3-164f3c4ac745"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: A médica estava cansada, estava ocupada de manhã.\n",
            "1: A médica estava cansada, estava toda manhã ocupada.\n",
            "2: O médico estava cansado, estava ocupado de manhã.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**output_scores** is whether or not to return the prediction scores. "
      ],
      "metadata": {
        "id": "Qx5YGWTBKZX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    output_scores=True,\n",
        "    return_dict_in_generate=True\n",
        ")\n",
        "\n",
        "for m in outputs:\n",
        "  print(m)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q-kowKjKhkP",
        "outputId": "dfadd672-f0de-4982-888a-2be6fbdc0ac5"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequences\n",
            "sequences_scores\n",
            "scores\n",
            "beam_indices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting of log probabilities of tokens conditioned \n",
        "#on log softmax of previously generated tokens in this beam. Tuple of torch.FloatTensor with up to max_new_tokens elements (one element for each generated\n",
        "# token), with each tensor of shape (batch_size*num_beams, config.vocab_size).\n",
        "outputs.scores "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7HThFiBMFwq",
        "outputId": "daf3182f-3b89-46d5-ce5e-c9c641c56207"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-12.5388, -11.3426, -15.5187,  ..., -26.4714, -26.4331, -26.4106],\n",
              "         [-12.5388, -11.3426, -15.5187,  ..., -26.4714, -26.4331, -26.4106],\n",
              "         [-12.5388, -11.3426, -15.5187,  ..., -26.4714, -26.4331, -26.4106],\n",
              "         ...,\n",
              "         [-12.5388, -11.3426, -15.5187,  ..., -26.4714, -26.4331, -26.4106],\n",
              "         [-12.5388, -11.3426, -15.5187,  ..., -26.4714, -26.4331, -26.4106],\n",
              "         [-12.5388, -11.3426, -15.5187,  ..., -26.4714, -26.4331, -26.4106]]),\n",
              " tensor([[-13.7348, -11.6968, -18.6643,  ..., -32.3291, -32.2599, -32.2011],\n",
              "         [-13.7489, -11.4951, -18.1527,  ..., -30.9017, -30.8568, -30.7965],\n",
              "         [-11.3940,  -8.6737, -14.6180,  ..., -28.1367, -28.0599, -28.1081],\n",
              "         ...,\n",
              "         [ -5.9505,  -8.7642, -14.5758,  ..., -26.3377, -26.2707, -26.2382],\n",
              "         [-13.4717,  -9.3911, -15.2133,  ..., -27.8848, -27.8123, -27.8238],\n",
              "         [-12.6452, -10.9745, -16.1720,  ..., -29.2740, -29.2042, -29.2129]]),\n",
              " tensor([[-11.1814, -11.7901, -16.8562,  ..., -30.0993, -29.9982, -29.9952],\n",
              "         [-11.0131, -11.6042, -16.7652,  ..., -30.2007, -30.1157, -30.1006],\n",
              "         [-11.1835, -11.8037, -16.6267,  ..., -29.9776, -29.8585, -29.8693],\n",
              "         ...,\n",
              "         [-12.4815, -11.0770, -16.2920,  ..., -27.2495, -27.1433, -27.1634],\n",
              "         [-14.7809, -13.3653, -18.9367,  ..., -29.9402, -29.7705, -29.7998],\n",
              "         [-11.3230, -11.7340, -16.7241,  ..., -30.2032, -30.1017, -30.1015]]),\n",
              " tensor([[-17.3803, -11.4013, -18.1454,  ..., -31.6341, -31.5414, -31.5451],\n",
              "         [-14.9713, -11.2689, -18.3731,  ..., -31.7046, -31.6245, -31.6113],\n",
              "         [-15.5771, -10.9024, -17.4581,  ..., -29.9471, -29.8510, -29.8742],\n",
              "         ...,\n",
              "         [-14.7618, -10.9216, -17.9469,  ..., -31.2238, -31.1358, -31.1497],\n",
              "         [-16.7920, -11.9032, -16.7106,  ..., -31.4953, -31.3996, -31.4775],\n",
              "         [-16.9605, -11.9603, -16.6980,  ..., -31.5198, -31.4250, -31.5045]]),\n",
              " tensor([[-20.7368, -15.2515, -20.0575,  ..., -36.0087, -35.9204, -35.8146],\n",
              "         [-18.1325, -15.9821, -20.6707,  ..., -37.9614, -37.9000, -37.7529],\n",
              "         [-19.4089, -14.7793, -19.2510,  ..., -34.6066, -34.5181, -34.4274],\n",
              "         ...,\n",
              "         [-18.4437, -16.2977, -20.9863,  ..., -38.4595, -38.3933, -38.2533],\n",
              "         [-17.3922, -15.0473, -18.9483,  ..., -33.2187, -33.1253, -33.1499],\n",
              "         [-17.5451, -14.8586, -18.8742,  ..., -33.0195, -32.9175, -32.9449]]),\n",
              " tensor([[-13.6698, -11.5048, -16.4766,  ..., -33.4826, -33.3126, -33.2958],\n",
              "         [-13.6326, -11.3368, -16.3966,  ..., -33.6664, -33.5176, -33.4943],\n",
              "         [-13.7723, -11.4981, -16.3042,  ..., -33.1288, -32.9514, -32.9401],\n",
              "         ...,\n",
              "         [-13.7453, -11.3887, -16.4117,  ..., -33.4584, -33.3024, -33.2912],\n",
              "         [-14.0773, -11.4879, -16.3754,  ..., -33.4894, -33.3428, -33.3157],\n",
              "         [-13.4696,  -9.8789, -16.7912,  ..., -30.5177, -30.4324, -30.4301]]),\n",
              " tensor([[-12.8628, -11.0177, -15.0492,  ..., -26.5417, -26.4687, -26.4350],\n",
              "         [-12.5962, -10.9558, -15.0012,  ..., -26.5451, -26.4672, -26.4295],\n",
              "         [-12.5216, -11.0201, -15.0087,  ..., -26.5057, -26.4318, -26.3939],\n",
              "         ...,\n",
              "         [-11.7888, -10.6035, -14.7768,  ..., -26.1402, -26.0640, -26.0240],\n",
              "         [-12.8232, -10.3144, -14.7539,  ..., -26.5670, -26.4920, -26.4513],\n",
              "         [-12.7209, -11.0080, -15.0208,  ..., -26.5441, -26.4701, -26.4346]]),\n",
              " tensor([[-13.7964,  -8.6851, -14.5556,  ..., -26.3488, -26.2997, -26.2701],\n",
              "         [-13.1214,  -9.3916, -15.4309,  ..., -27.2617, -27.2119, -27.1656],\n",
              "         [-12.9583,  -9.5496, -14.4171,  ..., -26.7011, -26.6427, -26.6844],\n",
              "         ...,\n",
              "         [-13.6393, -10.4537, -15.5789,  ..., -27.7928, -27.7340, -27.7148],\n",
              "         [-14.3319, -10.4441, -15.5855,  ..., -27.8896, -27.8356, -27.8205],\n",
              "         [-13.9097, -10.0593, -15.9719,  ..., -29.9804, -29.8602, -29.9071]]),\n",
              " tensor([[-12.6775,  -8.6917, -16.6581,  ..., -30.8023, -30.6888, -30.6435],\n",
              "         [-14.2786, -10.2979, -18.1673,  ..., -31.3009, -31.2228, -31.1421],\n",
              "         [-12.0137,  -8.8852, -16.2127,  ..., -30.1485, -30.0163, -29.9776],\n",
              "         ...,\n",
              "         [-15.7445, -10.2057, -17.8815,  ..., -31.0147, -30.9446, -30.8779],\n",
              "         [-13.2802,  -8.7253, -14.5814,  ..., -25.9444, -25.9000, -25.8697],\n",
              "         [-12.1862,  -9.7647, -15.6125,  ..., -26.8654, -26.8190, -26.7780]]),\n",
              " tensor([[-20.3316, -16.8010, -25.9062,  ..., -37.7654, -37.7138, -37.6644],\n",
              "         [-12.5347,  -9.2425, -14.9626,  ..., -29.2648, -29.1951, -29.1403],\n",
              "         [-19.7157, -16.5538, -25.3624,  ..., -37.5939, -37.5278, -37.4732],\n",
              "         ...,\n",
              "         [-13.1561,  -8.7731, -16.8371,  ..., -30.9581, -30.8324, -30.7931],\n",
              "         [-12.6760,  -8.6834, -16.6325,  ..., -30.6097, -30.5027, -30.4568],\n",
              "         [-15.3584, -13.0122, -18.8361,  ..., -31.9481, -31.8493, -31.8272]]),\n",
              " tensor([[-15.7530,  -7.8308, -17.1633,  ..., -32.6344, -32.5045, -32.4863],\n",
              "         [-16.1527,  -8.2266, -16.0350,  ..., -33.5347, -33.4321, -33.3844],\n",
              "         [-16.0331,  -7.6895, -17.2457,  ..., -33.0083, -32.8689, -32.8578],\n",
              "         ...,\n",
              "         [-20.1888, -16.5716, -25.7149,  ..., -37.5744, -37.5223, -37.4746],\n",
              "         [-18.8779, -16.0999, -23.3906,  ..., -37.0470, -36.9756, -36.9338],\n",
              "         [-20.4760, -16.9503, -26.0590,  ..., -38.0814, -38.0237, -37.9800]]),\n",
              " tensor([[-1.7499e+01, -8.3840e-04, -1.6852e+01,  ..., -3.2416e+01,\n",
              "          -3.2330e+01, -3.2273e+01],\n",
              "         [-1.7187e+01, -9.3857e-04, -1.6798e+01,  ..., -3.2318e+01,\n",
              "          -3.2231e+01, -3.2170e+01],\n",
              "         [-1.7407e+01, -8.2340e-04, -1.6852e+01,  ..., -3.2428e+01,\n",
              "          -3.2343e+01, -3.2287e+01],\n",
              "         ...,\n",
              "         [-1.4662e+01, -8.2023e+00, -1.7574e+01,  ..., -3.4829e+01,\n",
              "          -3.4745e+01, -3.4684e+01],\n",
              "         [-1.6329e+01, -7.9424e+00, -1.7310e+01,  ..., -3.2741e+01,\n",
              "          -3.2606e+01, -3.2591e+01],\n",
              "         [-1.7962e+01, -1.5247e+01, -2.3485e+01,  ..., -3.4913e+01,\n",
              "          -3.4853e+01, -3.4813e+01]]),\n",
              " tensor([[-1.7377e+01, -9.4560e-04, -1.6684e+01,  ..., -3.2179e+01,\n",
              "          -3.2094e+01, -3.2038e+01],\n",
              "         [-1.7331e+01, -8.7473e-04, -1.6774e+01,  ..., -3.2382e+01,\n",
              "          -3.2298e+01, -3.2244e+01],\n",
              "         [-1.7440e+01, -8.0875e-04, -1.6888e+01,  ..., -3.2559e+01,\n",
              "          -3.2474e+01, -3.2416e+01],\n",
              "         ...,\n",
              "         [-1.6165e+01, -8.1745e+00, -1.6118e+01,  ..., -3.2666e+01,\n",
              "          -3.2554e+01, -3.2507e+01],\n",
              "         [-1.8847e+01, -8.2889e+00, -1.6861e+01,  ..., -3.4812e+01,\n",
              "          -3.4680e+01, -3.4663e+01],\n",
              "         [-1.9210e+01, -7.9748e+00, -1.6757e+01,  ..., -3.4947e+01,\n",
              "          -3.4811e+01, -3.4794e+01]]),\n",
              " tensor([[-1.7335e+01, -1.1146e-03, -1.6665e+01,  ..., -3.1968e+01,\n",
              "          -3.1889e+01, -3.1829e+01],\n",
              "         [-1.6912e+01, -1.0157e-03, -1.6425e+01,  ..., -3.2559e+01,\n",
              "          -3.2470e+01, -3.2418e+01],\n",
              "         [-1.7032e+01, -9.3762e-04, -1.6485e+01,  ..., -3.2762e+01,\n",
              "          -3.2672e+01, -3.2620e+01],\n",
              "         ...,\n",
              "         [-1.6289e+01, -9.6047e+00, -1.7372e+01,  ..., -3.3021e+01,\n",
              "          -3.2920e+01, -3.2880e+01],\n",
              "         [-1.6561e+01, -8.4717e+00, -1.5804e+01,  ..., -3.2495e+01,\n",
              "          -3.2383e+01, -3.2325e+01],\n",
              "         [-1.6141e+01, -9.8056e+00, -1.6957e+01,  ..., -3.1126e+01,\n",
              "          -3.0948e+01, -3.0934e+01]]),\n",
              " tensor([[-15.3193, -10.4211, -19.8924,  ..., -30.4907, -30.2906, -30.3299],\n",
              "         [-15.3915, -10.4825, -20.0725,  ..., -30.7547, -30.5477, -30.5879],\n",
              "         [-16.5188, -11.1041, -18.8600,  ..., -32.5036, -32.4218, -32.4077],\n",
              "         ...,\n",
              "         [-14.7642,  -8.5984, -15.5645,  ..., -29.6421, -29.5386, -29.4895],\n",
              "         [-15.2248,  -7.5066, -15.3274,  ..., -30.3038, -30.2009, -30.2084],\n",
              "         [-15.7543,  -7.8876, -17.6225,  ..., -34.4886, -34.4143, -34.3786]]))"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.sequences_scores #final beam scores of the generated sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3_3432mOJyF",
        "outputId": "ba61f7a2-f7c3-4f95-e34d-c653e8466e1c"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3082, -0.3631, -0.3802])"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=5,\n",
        "    output_scores=True,\n",
        "    return_dict_in_generate=True\n",
        ")\n",
        "\n",
        "outputs.sequences_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1-XuXENOxts",
        "outputId": "f2789178-e661-43b3-b87e-f273bfca012c"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3082, -0.3631, -0.3802, -0.3894, -0.4312])"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**return_dict_in_generate** is whether or not to return a ModelOutput instead of a plain tuple."
      ],
      "metadata": {
        "id": "pquccE75Qqc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=5,\n",
        "    return_dict_in_generate=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "buxzY9e0RA4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl8aFtKVRWIo",
        "outputId": "b98701d5-e020-4586-f985-35ef10543799"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function BeamSearchEncoderDecoderOutput.values>"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diverse beam search decoding\n",
        "By calling group_beam_search(), if num_beams>1 and num_beam_groups>1"
      ],
      "metadata": {
        "id": "DEv41AB9TwMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    LogitsProcessorList,\n",
        "    MinLengthLogitsProcessor,\n",
        "    HammingDiversityLogitsProcessor,\n",
        "    BeamSearchScorer,\n",
        ")\n",
        "import torch\n",
        "\n",
        "encoder_input_str = \"She is a great doctor, but he is a bad patient.\"\n",
        "encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "# lets run diverse beam search using 6 beams\n",
        "num_beams = 6\n",
        "# define decoder start token ids\n",
        "input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
        "input_ids = input_ids * model.config.decoder_start_token_id\n",
        "\n",
        "# add encoder_outputs to model keyword arguments\n",
        "model_kwargs = {\n",
        "    \"encoder_outputs\": model.get_encoder()(\n",
        "        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
        "    )\n",
        "}\n",
        "\n",
        "# instantiate beam scorer\n",
        "beam_scorer = BeamSearchScorer(\n",
        "    batch_size=1, # Batch Size of input_ids for which standard beam search decoding is run in parallel.\n",
        "    max_length=model.config.max_length, # The maximum length of the sequence to be generated\n",
        "    num_beams=num_beams, # Number of beams for beam search.\n",
        "    device=model.device, # Defines the device type (e.g., \"cpu\" or \"cuda\") on which this instance of BeamSearchScorer will be allocated.\n",
        "    num_beam_groups=3, # Number of groups to divide num_beams into in order to ensure diversity among different groups of beams.\n",
        "    num_beam_hyps_to_keep=3, # The number of beam hypotheses that shall be returned upon calling finalize.\n",
        ")\n",
        "\n",
        "# instantiate logits processors\n",
        "logits_processor = LogitsProcessorList( # A LogitsProcessor can be used to modify the prediction scores of a language model head for generation.\n",
        "    [\n",
        "        HammingDiversityLogitsProcessor(diversity_penalty=5.5, num_beams=6, num_beam_groups=3), # LogitsProcessor that enforces diverse beam search. \n",
        "        MinLengthLogitsProcessor(min_length=5, eos_token_id=model.config.eos_token_id), # LogitsProcessor enforcing a min-length by setting EOS probability to 0.\n",
        "    ]\n",
        ")\n",
        "\n",
        "outputs = model.group_beam_search(\n",
        "    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs\n",
        ")\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DRRp5yiUH6f",
        "outputId": "150b951b-f29c-46b0-a809-5d8d40bfbc52"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ela é um grande médico, mas ele é um doente ruim.',\n",
              " 'Ela é um grande médico, mas ele é um mau paciente.',\n",
              " 'Ela é um grande médico, mas ele é um mau paciente...']"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contrained beam search\n",
        "\n",
        "Generates sequences of token ids for models with a language modeling head using constrained beam search decoding and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
        "\n"
      ],
      "metadata": {
        "id": "eUNdw8SAnjv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    LogitsProcessorList,\n",
        "    MinLengthLogitsProcessor,\n",
        "    ConstrainedBeamSearchScorer,\n",
        "    PhrasalConstraint,\n",
        ")\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"VanessaSchenkel/pt-unicamp-news-t5\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"VanessaSchenkel/pt-unicamp-news-t5\")\n",
        "\n",
        "encoder_input_str = \"She is a great doctor, but he is a bad patient.\"\n",
        "encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "# lets run beam search using 3 beams\n",
        "num_beams = 3\n",
        "# define decoder start token ids\n",
        "input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
        "input_ids = input_ids * model.config.decoder_start_token_id\n",
        "\n",
        "# add encoder_outputs to model keyword arguments\n",
        "model_kwargs = {\n",
        "    \"encoder_outputs\": model.get_encoder()(\n",
        "        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
        "    )\n",
        "}\n",
        "\n",
        "constraint_str = \"paciente\"\n",
        "constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token\n",
        "constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]\n",
        "\n",
        "\n",
        "# instantiate beam scorer\n",
        "beam_scorer = ConstrainedBeamSearchScorer(\n",
        "    batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints, num_beam_hyps_to_keep=3\n",
        ")\n",
        "\n",
        "# instantiate logits processors\n",
        "logits_processor = LogitsProcessorList(\n",
        "    [\n",
        "        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
        "    ]\n",
        ")\n",
        "\n",
        "outputs = model.constrained_beam_search(\n",
        "    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs\n",
        ")\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LirdmO1gnpJR",
        "outputId": "809d47fc-b9a3-4e7f-89cf-0d6e7712e99d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:3174: UserWarning: You don't have defined any stopping_criteria, this will likely loop forever\n",
            "  warnings.warn(\"You don't have defined any stopping_criteria, this will likely loop forever\", UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ela é um grande médico, mas ele é um mau paciente.',\n",
              " 'Ela é um grande médico, mas ele é um paciente ruim.',\n",
              " 'Ela é um grande médico, mas ele é um mau paciente...']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PhrasalConstraint\n",
        "Constraint enforcing that an ordered sequence of tokens is included in the output."
      ],
      "metadata": {
        "id": "0QOeXU0Mfbek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constraints = [\n",
        "    PhrasalConstraint(\n",
        "        tokenizer(\"grande médica\", add_special_tokens=False).input_ids\n",
        "    )\n",
        "]\n",
        "\n",
        "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    constraints=constraints,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3,\n",
        "    remove_invalid_values=True,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "for sentence in outputs:\n",
        "    print(tokenizer.decode(sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddwWS7E2fxHS",
        "outputId": "11a9a275-e8a3-4fdf-b4bc-b2250f4ec3e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ela é um grande médico, mas ele é um doente ruim.......\n",
            "Ela é um grande médico, mas ele é um doente ruim...... e\n",
            "Ela é um grande médico, mas ele é um doente ruim...... grande\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DisjunctiveConstraint\n",
        "A special Constraint that is fulfilled by fulfilling just one of several constraints.\n",
        "Allow the user to input a list of words, whose purpose is to guide the generation such that the final output must contain just at least one among the list of words."
      ],
      "metadata": {
        "id": "cp7axaLdgjdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DisjunctiveConstraint\n",
        "\n",
        "encoder_input_str = \"She is a great doctor, but he is a bad patient.\"\n",
        "encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "# lets run beam search using 3 beams\n",
        "num_beams = 3\n",
        "# define decoder start token ids\n",
        "input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
        "input_ids = input_ids * model.config.decoder_start_token_id\n",
        "\n",
        "# add encoder_outputs to model keyword arguments\n",
        "model_kwargs = {\n",
        "    \"encoder_outputs\": model.get_encoder()(\n",
        "        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
        "    )\n",
        "}\n",
        "\n",
        "constraint_str = [\"médico\", \"médica\"]\n",
        "\n",
        "constraints = [\n",
        "    DisjunctiveConstraint(\n",
        "        tokenizer(constraint_str, add_special_tokens=False).input_ids\n",
        "    )\n",
        "]\n",
        "\n",
        "# instantiate beam scorer\n",
        "beam_scorer = ConstrainedBeamSearchScorer(\n",
        "    batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints, num_beam_hyps_to_keep=3\n",
        ")\n",
        "\n",
        "# instantiate logits processors\n",
        "logits_processor = LogitsProcessorList(\n",
        "    [\n",
        "        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
        "    ]\n",
        ")\n",
        "\n",
        "outputs = model.constrained_beam_search(\n",
        "    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs\n",
        ")\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1vHeyefiAg1",
        "outputId": "85663654-66e0-4af6-a67f-3d222f9180a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:3174: UserWarning: You don't have defined any stopping_criteria, this will likely loop forever\n",
            "  warnings.warn(\"You don't have defined any stopping_criteria, this will likely loop forever\", UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ela é um grande médico, mas ele é um doente ruim.',\n",
              " 'Ela é um grande médico, mas ele é um mau paciente.',\n",
              " 'Ela é um grande médico, mas é um doente ruim.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outputs = model.constrained_beam_search(\n",
        "    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, return_dict_in_generate=True, output_scores=True, **model_kwargs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4lX_PLulqgd",
        "outputId": "24079306-a835-4c1e-871c-f74b21a73d10"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:3174: UserWarning: You don't have defined any stopping_criteria, this will likely loop forever\n",
            "  warnings.warn(\"You don't have defined any stopping_criteria, this will likely loop forever\", UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.sequences_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HE8J4Gyl0CU",
        "outputId": "e1799de9-be7a-4c8a-ccd1-0f816df34645"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.2165, -0.2186, -0.2516])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "force_flexible = [\"médica\", \"doutora\"]\n",
        "\n",
        "force_flexible_ids = [\n",
        "    tokenizer(force_flexible, add_special_tokens=False).input_ids,\n",
        "]\n",
        "\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    force_words_ids=force_flexible_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=1,\n",
        "    remove_invalid_values=True,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "for sentence in outputs:\n",
        "    print(tokenizer.decode(sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDHipstTemVw",
        "outputId": "ece27ca3-671a-4005-b145-4ce2b8efc69d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ela é um grande médico, mas ele tem uma má paciente. A sua maioria não está médica\n",
            "É um grande médico, mas ele é uma doente ruim. O seu tratamento está a ser médica\n",
            "Ela é um grande médico, mas ele tem uma má paciente. A sua parte de risco médica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "force_flexible = [\"médica\", \"doutora\"]\n",
        "\n",
        "force_flexible_ids = [\n",
        "    tokenizer(force_flexible, add_special_tokens=False).input_ids,\n",
        "]\n",
        "\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    force_words_ids=force_flexible_ids,\n",
        "    num_beams=5,\n",
        "    num_return_sequences=3,\n",
        "    no_repeat_ngram_size=10,\n",
        "    remove_invalid_values=True,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "for sentence in outputs:\n",
        "    print(tokenizer.decode(sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xub20kMkcUMH",
        "outputId": "666ea5e9-f4e5-42a6-840f-15dcda2c94fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "No No No, a experiência com o\n",
            "No No No, a experiência com a\n",
            "No No No, a experiência com doutor\n",
            "No No No, a experiência com o\n",
            "No No No, a experiência com a\n",
            "No No No, a experiência com doutor\n",
            "No No No, a experiência com o\n",
            "No No No, a experiência com a\n",
            "No No No, a experiência com doutor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IlDbDWSTnxUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import numpy as np\n",
        "import math\n",
        "import heapq\n",
        " \n",
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "    \"\"\"\n",
        "    data: (n, m) where n is number of words in sequence.\n",
        "        and m is number of classes (words in target vocab).\n",
        "    k: beam search parameter\n",
        "    \"\"\"\n",
        "    sequences = [[[], 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data: # ----> n\n",
        "        all_candidates = []\n",
        "        # find the indexes of k largest probabilities in the row\n",
        "        k_largest = heapq.nlargest(k, range(len(row)), row.take) # -----> m\n",
        "        # expand each current candidate\n",
        "        for seq, score in sequences: # ----> k\n",
        "            for j in k_largest: # -----> k\n",
        "                s = score - math.log(row[j])\n",
        "                candidate = [seq + [j], s]\n",
        "                all_candidates.append(candidate)\n",
        "        # sort all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1]) # -----> k log k\n",
        "        # select best k\n",
        "        sequences = ordered[:k]\n",
        "    return sequences\n",
        " \n",
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t[0.5, 0.4, 0.3, 0.2, 0.1]]\n",
        "data = array(data)\n",
        "# decode sequence\n",
        "result = beam_search_decoder(data, 3)\n",
        "# print result\n",
        "for seq in result:\n",
        "\tprint(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3_iemY4sFUF",
        "outputId": "c384f1b9-4632-4d35-f06f-cae3bffb8b69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]\n"
          ]
        }
      ]
    }
  ]
}